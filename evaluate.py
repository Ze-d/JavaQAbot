import os
import ast
import pandas as pd
from datasets import Dataset
from agent import Agent  # ä½ çš„ Agent ç±»
from ragas import evaluate
from ragas.metrics import (
    answer_correctness,
    answer_relevancy,
    faithfulness
)
from ragas.llms import LangchainLLMWrapper
from ragas.embeddings import LangchainEmbeddingsWrapper


TESTSET_FILE = "auto_generated_testset.csv"
EVALUATOR_MODEL = "deepseek-chat"



def parse_context(context_str):

    if pd.isna(context_str) or context_str == "":
        return []
    try:
        # æ¸…ç†æ¢è¡Œç¬¦å’Œå¤šä½™ç©ºæ ¼ï¼Œé¿å…è§£æé”™è¯¯
        clean_str = context_str.replace("\n", "").strip()
        return ast.literal_eval(clean_str)
    except (SyntaxError, ValueError):
        # è‹¥è§£æå¤±è´¥ï¼Œè¿”å›å•å…ƒç´ åˆ—è¡¨ï¼ˆåŒ…å«åŸå§‹å­—ç¬¦ä¸²ï¼‰
        return [str(context_str)]


def main():

    test_df = pd.read_csv(TESTSET_FILE,encoding='utf-8')
    required_columns = ["user_input", "reference", "reference_contexts"]
    test_df = test_df.dropna(subset=required_columns)

    agent = Agent()


    # æå–å…³é”®æ•°æ®
    questions = test_df["user_input"].tolist()
    ground_truths = [str(gt) for gt in test_df["reference"].tolist()]
    # è§£æ reference_contexts ä¸ºå®é™…åˆ—è¡¨ï¼ˆç”¨äºåç»­å¯¹æ¯”æ£€ç´¢æ•ˆæœï¼‰
    reference_contexts = [parse_context(ctx) for ctx in test_df["reference_contexts"].tolist()]

    generated_answers = []
    retrieved_contexts = []  # å­˜å‚¨ Agent å®é™…æ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡

    print(f" ({len(questions)} é¢˜)...")

    for i, q in enumerate(questions):
        print(f"\nğŸ“ [é¢˜ {i + 1}] {q}")

        # è°ƒç”¨ Agent å¹¶è·å–ç­”æ¡ˆå’Œæ£€ç´¢ä¸Šä¸‹æ–‡
        ans, ctx = agent.query(q, return_context=True)

        print(f"   ğŸ—£ï¸ ç­”: {ans[:50]}...")
        print(f"   ğŸ“š æ£€ç´¢åˆ° {len(ctx)} æ¡è¯æ®")

        generated_answers.append(str(ans))
        retrieved_contexts.append(ctx)


    # æ„å»º Ragas è¯„ä¼°æ•°æ®é›†
    dataset = Dataset.from_dict({
        "question": questions,
        "answer": generated_answers,
        "contexts": retrieved_contexts,  # Agent å®é™…æ£€ç´¢çš„ä¸Šä¸‹æ–‡
        "ground_truth": ground_truths,
        "reference_contexts": reference_contexts  # åŸå§‹å‚è€ƒä¸Šä¸‹æ–‡ï¼ˆå¯é€‰ï¼Œç”¨äºåˆ†æï¼‰
    })

    # é…ç½®è¯„ä¼°æ¨¡å‹ï¼ˆä½¿ç”¨ä½ çš„ utils å‡½æ•°è·å– LLM å’Œ Embeddingsï¼‰
    from utils import get_llm_model, get_embeddings_model  # ç¡®ä¿å¯¼å…¥æ­£ç¡®
    llm = get_llm_model()
    embedding_function = get_embeddings_model()

    evaluator_llm = LangchainLLMWrapper(llm)
    evaluator_embeddings = LangchainEmbeddingsWrapper(embedding_function)

    # é€‰æ‹©è¯„ä¼°æŒ‡æ ‡
    metrics = [
        answer_correctness,  # ç­”æ¡ˆå‡†ç¡®ç‡ï¼ˆä¸ ground_truth å¯¹æ¯”ï¼‰
        answer_relevancy,  # ç­”æ¡ˆç›¸å…³æ€§ï¼ˆä¸é—®é¢˜çš„åŒ¹é…åº¦ï¼‰
        faithfulness  # å¿ å®åº¦ï¼ˆç­”æ¡ˆæ˜¯å¦åŸºäºæ£€ç´¢ä¸Šä¸‹æ–‡ï¼Œæ— å¹»è§‰ï¼‰
    ]

    print(f"\nâš–ï¸ æ­£åœ¨è¿›è¡Œå…¨ç»´åº¦æ‰“åˆ† (å«å¹»è§‰æ£€æµ‹)...")
    results = evaluate(
        dataset=dataset,
        metrics=metrics,
        llm=evaluator_llm,
        embeddings=evaluator_embeddings
    )

    print("\n====== ğŸ† å®Œæ•´è¯„ä¼°æŠ¥å‘Š ======")
    print(results)

    # ä¿å­˜ç»“æœï¼ˆåŒ…å«åŸå§‹æ•°æ®å’Œè¯„ä¼°åˆ†æ•°ï¼‰
    result_df = results.to_pandas()
    # åˆå¹¶åŸå§‹æµ‹è¯•é›†æ•°æ®ï¼Œæ–¹ä¾¿åˆ†æ
    original_df = test_df.reset_index(drop=True)
    final_df = pd.concat([original_df, result_df], axis=1)
    final_df.to_excel("full_evaluation_report.xlsx", index=False)

    print("âœ… æŠ¥å‘Šå·²ä¿å­˜è‡³ full_evaluation_report.xlsx")


if __name__ == "__main__":
    main()
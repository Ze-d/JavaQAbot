"""
æ¨¡å‹è¯„ä¼°æ¨¡å—
ä½œè€…ï¼šzjy
åˆ›å»ºæ—¶é—´ï¼š2024å¹´

è¯¥æ¨¡å—ä½¿ç”¨Ragasæ¡†æ¶å¯¹åŒ»ç–—é—®è¯ŠAgentè¿›è¡Œå…¨é¢è¯„ä¼°ï¼ŒåŒ…æ‹¬ï¼š
1. åŠ è½½æµ‹è¯•æ•°æ®é›†
2. è¿è¡ŒAgentç”Ÿæˆå›ç­”
3. ä½¿ç”¨å¤šä¸ªè¯„ä¼°æŒ‡æ ‡ï¼ˆå‡†ç¡®ç‡ã€ç›¸å…³æ€§ã€å¿ å®åº¦ï¼‰
4. ç”Ÿæˆè¯¦ç»†çš„è¯„ä¼°æŠ¥å‘Š

è¯„ä¼°æŒ‡æ ‡ï¼š
- answer_correctness: ç­”æ¡ˆå‡†ç¡®ç‡ï¼ˆä¸æ ‡å‡†ç­”æ¡ˆå¯¹æ¯”ï¼‰
- answer_relevancy: ç­”æ¡ˆç›¸å…³æ€§ï¼ˆä¸é—®é¢˜çš„åŒ¹é…åº¦ï¼‰
- faithfulness: å¿ å®åº¦ï¼ˆç­”æ¡ˆæ˜¯å¦åŸºäºæ£€ç´¢ä¸Šä¸‹æ–‡ï¼Œæ— å¹»è§‰ï¼‰

è¾“å‡ºï¼š
- æ§åˆ¶å°æ˜¾ç¤ºè¯„ä¼°ç»“æœ
- Excelæ–‡ä»¶ä¿å­˜è¯¦ç»†æŠ¥å‘Š
"""

import ast
import os
from typing import List

import pandas as pd
from datasets import Dataset
from ragas import evaluate
from ragas.llms import LangchainLLMWrapper
from ragas.embeddings import LangchainEmbeddingsWrapper
from ragas.metrics import (
    answer_correctness,
    answer_relevancy,
    faithfulness
)

from agent import Agent
from utils import get_llm_model, get_embeddings_model


# é…ç½®å‚æ•°
TESTSET_FILE = "auto_generated_testset.csv"
EVALUATOR_MODEL = "deepseek-chat"


def parse_context(context_str: str) -> List[str]:
    """
    è§£æä¸Šä¸‹æ–‡å­—ç¬¦ä¸²ä¸ºåˆ—è¡¨

    å°†CSVä¸­å­˜å‚¨çš„å­—ç¬¦ä¸²æ ¼å¼ä¸Šä¸‹æ–‡è½¬æ¢ä¸ºPythonåˆ—è¡¨ã€‚

    Args:
        context_str (str): ä¸Šä¸‹æ–‡å­—ç¬¦ä¸²ï¼Œå¯èƒ½åŒ…å«åˆ—è¡¨æ ¼å¼

    Returns:
        List[str]: è§£æåçš„ä¸Šä¸‹æ–‡åˆ—è¡¨
    """
    # å¤„ç†ç©ºå€¼æˆ–ç©ºå­—ç¬¦ä¸²
    if pd.isna(context_str) or context_str == "":
        return []

    try:
        # æ¸…ç†æ¢è¡Œç¬¦å’Œå¤šä½™ç©ºæ ¼ï¼Œé¿å…è§£æé”™è¯¯
        clean_str = context_str.replace("\n", "").strip()
        return ast.literal_eval(clean_str)
    except (SyntaxError, ValueError):
        # è‹¥è§£æå¤±è´¥ï¼Œè¿”å›å•å…ƒç´ åˆ—è¡¨ï¼ˆåŒ…å«åŸå§‹å­—ç¬¦ä¸²ï¼‰
        return [str(context_str)]


def main():
    """
    ä¸»è¯„ä¼°æµç¨‹

    æ‰§è¡Œä»¥ä¸‹æ­¥éª¤ï¼š
    1. åŠ è½½æµ‹è¯•æ•°æ®é›†
    2. åˆå§‹åŒ–Agentå’Œè¯„ä¼°æ¨¡å‹
    3. å¯¹æ¯ä¸ªé—®é¢˜ç”Ÿæˆå›ç­”å’Œæ£€ç´¢ä¸Šä¸‹æ–‡
    4. ä½¿ç”¨Ragasæ¡†æ¶è¿›è¡Œå…¨é¢è¯„ä¼°
    5. ç”Ÿæˆå¹¶ä¿å­˜è¯„ä¼°æŠ¥å‘Š
    """
    print("=" * 60)
    print("ğŸ¥ åŒ»ç–—é—®è¯ŠAgentè¯„ä¼°ç¨‹åºå¯åŠ¨")
    print("=" * 60)

    # 1. åŠ è½½æµ‹è¯•æ•°æ®é›†
    print("\nğŸ“Š åŠ è½½æµ‹è¯•æ•°æ®é›†...")
    test_df = pd.read_csv(TESTSET_FILE, encoding='utf-8')
    required_columns = ["user_input", "reference", "reference_contexts"]
    test_df = test_df.dropna(subset=required_columns)
    print(f"âœ… æˆåŠŸåŠ è½½ {len(test_df)} æ¡æµ‹è¯•æ•°æ®")

    # 2. åˆå§‹åŒ–Agent
    print("\nğŸ¤– åˆå§‹åŒ–Agentå®ä¾‹...")
    agent = Agent()

    # 3. æå–å…³é”®æ•°æ®
    questions = test_df["user_input"].tolist()
    ground_truths = [str(gt) for gt in test_df["reference"].tolist()]
    # è§£æ reference_contexts ä¸ºå®é™…åˆ—è¡¨ï¼ˆç”¨äºåç»­å¯¹æ¯”æ£€ç´¢æ•ˆæœï¼‰
    reference_contexts = [parse_context(ctx) for ctx in test_df["reference_contexts"].tolist()]

    generated_answers = []
    retrieved_contexts = []  # å­˜å‚¨ Agent å®é™…æ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡

    print(f"\nğŸ’¬ å¼€å§‹ç”Ÿæˆå›ç­” (å…± {len(questions)} é¢˜)...")

    # 4. éå†æ‰€æœ‰é—®é¢˜ï¼Œç”Ÿæˆå›ç­”
    for i, q in enumerate(questions):
        print(f"\nğŸ“ [é¢˜ {i + 1}] {q}")

        # è°ƒç”¨ Agent å¹¶è·å–ç­”æ¡ˆå’Œæ£€ç´¢ä¸Šä¸‹æ–‡
        ans, ctx = agent.query(q, return_context=True)

        print(f"   ğŸ—£ï¸ ç­”: {ans[:50]}...")
        print(f"   ğŸ“š æ£€ç´¢åˆ° {len(ctx)} æ¡è¯æ®")

        generated_answers.append(str(ans))
        retrieved_contexts.append(ctx)

    # 5. æ„å»º Ragas è¯„ä¼°æ•°æ®é›†
    print("\nğŸ”§ æ„å»ºRagasè¯„ä¼°æ•°æ®é›†...")
    dataset = Dataset.from_dict({
        "question": questions,
        "answer": generated_answers,
        "contexts": retrieved_contexts,  # Agent å®é™…æ£€ç´¢çš„ä¸Šä¸‹æ–‡
        "ground_truth": ground_truths,
        "reference_contexts": reference_contexts  # åŸå§‹å‚è€ƒä¸Šä¸‹æ–‡ï¼ˆå¯é€‰ï¼Œç”¨äºåˆ†æï¼‰
    })

    # 6. é…ç½®è¯„ä¼°æ¨¡å‹
    print("\nâš™ï¸ é…ç½®è¯„ä¼°æ¨¡å‹...")
    llm = get_llm_model()
    embedding_function = get_embeddings_model()

    evaluator_llm = LangchainLLMWrapper(llm)
    evaluator_embeddings = LangchainEmbeddingsWrapper(embedding_function)

    # 7. é€‰æ‹©è¯„ä¼°æŒ‡æ ‡
    metrics = [
        answer_correctness,  # ç­”æ¡ˆå‡†ç¡®ç‡ï¼ˆä¸ ground_truth å¯¹æ¯”ï¼‰
        answer_relevancy,  # ç­”æ¡ˆç›¸å…³æ€§ï¼ˆä¸é—®é¢˜çš„åŒ¹é…åº¦ï¼‰
        faithfulness  # å¿ å®åº¦ï¼ˆç­”æ¡ˆæ˜¯å¦åŸºäºæ£€ç´¢ä¸Šä¸‹æ–‡ï¼Œæ— å¹»è§‰ï¼‰
    ]

    # 8. æ‰§è¡Œè¯„ä¼°
    print(f"\nâš–ï¸ æ­£åœ¨è¿›è¡Œå…¨ç»´åº¦æ‰“åˆ† (å«å¹»è§‰æ£€æµ‹)...")
    results = evaluate(
        dataset=dataset,
        metrics=metrics,
        llm=evaluator_llm,
        embeddings=evaluator_embeddings
    )

    # 9. æ˜¾ç¤ºè¯„ä¼°ç»“æœ
    print("\n" + "=" * 60)
    print("ğŸ† å®Œæ•´è¯„ä¼°æŠ¥å‘Š")
    print("=" * 60)
    print(results)

    # 10. ä¿å­˜è¯¦ç»†æŠ¥å‘Š
    print("\nğŸ’¾ ä¿å­˜è¯¦ç»†è¯„ä¼°æŠ¥å‘Š...")
    result_df = results.to_pandas()
    # åˆå¹¶åŸå§‹æµ‹è¯•é›†æ•°æ®ï¼Œæ–¹ä¾¿åˆ†æ
    original_df = test_df.reset_index(drop=True)
    final_df = pd.concat([original_df, result_df], axis=1)
    final_df.to_excel("full_evaluation_report.xlsx", index=False)

    print("âœ… æŠ¥å‘Šå·²ä¿å­˜è‡³ full_evaluation_report.xlsx")
    print("\nğŸ‰ è¯„ä¼°å®Œæˆï¼")


if __name__ == "__main__":
    main()

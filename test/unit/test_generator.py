"""
æµ‹è¯•é›†ç”Ÿæˆå™¨æ¨¡å—
ä½œè€…ï¼šzjy
åˆ›å»ºæ—¶é—´ï¼š2024å¹´

è¯¥æ¨¡å—ä½¿ç”¨Ragasæ¡†æ¶ä»å‘é‡æ•°æ®åº“ç”Ÿæˆé«˜è´¨é‡çš„æµ‹è¯•æ•°æ®é›†ï¼ŒåŒ…å«ä»¥ä¸‹åŠŸèƒ½ï¼š
1. ä»Chromaå‘é‡æ•°æ®åº“åŠ è½½æ–‡æ¡£
2. ä½¿ç”¨K-Meansèšç±»ç­›é€‰ä»£è¡¨æ€§æ ·æœ¬ï¼ˆæé«˜æ•ˆç‡ï¼‰
3. åŸºäºLLMç”Ÿæˆå¤šæ ·åŒ–çš„é—®ç­”å¯¹
4. è¾“å‡ºä¸ºCSVæ ¼å¼çš„æµ‹è¯•é›†

ä¸»è¦ä¼˜åŒ–ç­–ç•¥ï¼š
- K-Meansèšç±»ï¼šä»å¤§é‡æ–‡æ¡£ä¸­ç­›é€‰ä»£è¡¨æ€§æ ·æœ¬
- æ–‡æ¡£åˆå¹¶ï¼šå°†å¤šä¸ªæ–‡æ¡£åˆå¹¶ä¸ºé•¿æ–‡æœ¬ï¼Œæé«˜ç”Ÿæˆç¨³å®šæ€§
- å…¼å®¹å¤„ç†ï¼šæ”¯æŒæ–°æ—§ç‰ˆæœ¬Ragasæ¡†æ¶çš„å…¼å®¹æ€§å¤„ç†

é…ç½®å‚æ•°ï¼š
- CHROMA_DB_DIR: å‘é‡æ•°æ®åº“è·¯å¾„
- TESTSET_FILE: è¾“å‡ºæµ‹è¯•é›†æ–‡ä»¶å
- N_CLUSTERS: K-Meansèšç±»æ•°é‡
- TESTSET_SIZE: ç”Ÿæˆçš„æµ‹è¯•é—®é¢˜æ•°é‡
"""

import os
import warnings
from typing import List

import numpy as np
import pandas as pd
from langchain_chroma import Chroma
from langchain_core.documents import Document
from ragas.embeddings import LangchainEmbeddingsWrapper
from ragas.llms import LangchainLLMWrapper
from ragas.testset import TestsetGenerator
from ragas.testset.synthesizers import QuestionGenerator
from sklearn.cluster import KMeans
from sklearn.metrics import pairwise_distances_argmin_min

from utils import get_llm_model, get_embeddings_model

# å¿½ç•¥è­¦å‘Šä¿¡æ¯
warnings.filterwarnings('ignore')

# ================= é…ç½®åŒºåŸŸ =================
CHROMA_DB_DIR = './data/db'
TESTSET_FILE = "auto_generated_testset.csv"
N_CLUSTERS = 200  # ç­›é€‰åçš„ä»£è¡¨æ€§æ–‡æ¡£æ•°
TESTSET_SIZE = 15  # ç”Ÿæˆçš„æµ‹è¯•é—®é¢˜æ•°
# =========================================


def main():
    """
    ä¸»å‡½æ•°ï¼šç”Ÿæˆæµ‹è¯•æ•°æ®é›†

    æµç¨‹ï¼š
    1. åˆå§‹åŒ–LLMå’ŒåµŒå…¥æ¨¡å‹
    2. ä»Chromaæ•°æ®åº“åŠ è½½æ–‡æ¡£
    3. ä½¿ç”¨K-Meansèšç±»ç­›é€‰ä»£è¡¨æ€§æ ·æœ¬
    4. åˆå¹¶æ–‡æ¡£ä¸ºé•¿æ–‡æœ¬ï¼ˆæé«˜ç¨³å®šæ€§ï¼‰
    5. ä½¿ç”¨Ragasç”Ÿæˆæµ‹è¯•é›†
    6. ä¿å­˜ä¸ºCSVæ–‡ä»¶
    """
    print("=" * 60)
    print("ğŸš€ æµ‹è¯•é›†ç”Ÿæˆå™¨å¯åŠ¨")
    print("=" * 60)

    # 1. åˆå§‹åŒ–æ¨¡å‹ï¼ˆå…¼å®¹æ—§ç‰ˆ Ragas Wrapperï¼‰
    print("\nğŸ”§ åˆå§‹åŒ– LLM å’ŒåµŒå…¥æ¨¡å‹...")
    llm = get_llm_model()
    embedding_function = get_embeddings_model()
    generator_llm = LangchainLLMWrapper(llm)
    generator_embeddings = LangchainEmbeddingsWrapper(embedding_function)

    # 2. ä» Chroma åŠ è½½æ•°æ®
    print("\nğŸ“– ä» Chroma æ•°æ®åº“åŠ è½½æ–‡æ¡£...")
    vectorstore = Chroma(
        persist_directory=CHROMA_DB_DIR,
        embedding_function=embedding_function
    )
    db_data = vectorstore.get()
    texts = db_data['documents']
    metadatas = db_data['metadatas']
    print(f"âœ… æˆåŠŸåŠ è½½ {len(texts)} ä¸ªæ–‡æ¡£ç‰‡æ®µã€‚")

    # 3. æ–‡æ¡£ç­›é€‰ï¼šK-Means èšç±»ï¼ˆæ ¸å¿ƒæé€Ÿï¼Œä¸å¯çœï¼‰
    if len(texts) > N_CLUSTERS:
        print(f"\nâš¡ æ–‡æ¡£æ•°é‡è¿‡å¤šï¼Œæ­£åœ¨ä½¿ç”¨ K-Means èšç±»ç­›é€‰ {N_CLUSTERS} ä¸ªä»£è¡¨æ€§æ–‡æ¡£...")

        all_doc_ids = db_data['ids']
        batch_size = 1000
        all_embeddings = []
        for i in range(0, len(all_doc_ids), batch_size):
            batch_ids = all_doc_ids[i:i + batch_size]
            batch_data = vectorstore.get(ids=batch_ids, include=['embeddings'])
            all_embeddings.extend(batch_data['embeddings'])

        embeddings_array = np.array(all_embeddings)
        kmeans = KMeans(n_clusters=N_CLUSTERS, random_state=42, n_init=10)
        cluster_labels = kmeans.fit_predict(embeddings_array)
        closest_indices, _ = pairwise_distances_argmin_min(kmeans.cluster_centers_, embeddings_array)

        selected_texts = [texts[i] for i in closest_indices]
        selected_metadatas = [metadatas[i] for i in closest_indices]
        print(f"âœ… ç­›é€‰å®Œæˆï¼Œå…±é€‰æ‹© {len(selected_texts)} ä¸ªä»£è¡¨æ€§æ–‡æ¡£ç‰‡æ®µã€‚")
    else:
        selected_texts = texts
        selected_metadatas = metadatas
        print("â„¹ï¸  æ–‡æ¡£æ•°é‡è¾ƒå°‘ï¼Œä½¿ç”¨å…¨éƒ¨æ–‡æ¡£ç”Ÿæˆæµ‹è¯•é›†ã€‚")

    # 4. ç»ˆææŠ€å·§ï¼šåˆå¹¶æ‰€æœ‰æ–‡æ¡£ä¸ºä¸€ä¸ªé•¿æ–‡æœ¬ï¼Œå½»åº•ç»•å¼€åˆ†æ®µå’Œ headlines ä¾èµ–
    print("\nğŸ“„ åˆå¹¶æ–‡æ¡£ä¸ºé•¿æ–‡æœ¬ï¼Œç»•å¼€æ‰€æœ‰åˆ†æ®µé€»è¾‘...")
    # æ—§ç‰ˆ Ragas å¯¹å•æ–‡æœ¬ç”Ÿæˆé—®é¢˜æ›´ç¨³å®šï¼Œæ— åˆ†æ®µä¾èµ–
    combined_text = "\n\n---\n\n".join(selected_texts)  # ç”¨åˆ†éš”ç¬¦åˆå¹¶æ‰€æœ‰æ–‡æ¡£
    # æ„é€ å•ä¸ª Documentï¼ˆæ— ä»»ä½•é¢å¤–å­—æ®µï¼Œé¿å…å…ƒæ•°æ®é—®é¢˜ï¼‰
    documents = [Document(page_content=combined_text, metadata={})]

    # 5. åˆå§‹åŒ– Ragas ç”Ÿæˆå™¨ï¼ˆæç®€æ¨¡å¼ï¼‰
    print("\nğŸ§  æ­£åœ¨ä½¿ç”¨ Ragas ç”Ÿæˆæµ‹è¯•é›†ï¼ˆç»ˆæå…¼å®¹æ¨¡å¼ï¼‰...")
    generator = TestsetGenerator(
        llm=generator_llm,
        embedding_model=generator_embeddings
    )

    # 6. ç”Ÿæˆæµ‹è¯•é›†ï¼ˆä»…ç”¨æ—§ç‰ˆæ”¯æŒçš„å‚æ•°ï¼Œä¸è§¦å‘ä»»ä½•åˆ†æ®µï¼‰
    try:
        # ç¬¬ä¸€æ¬¡å°è¯•ï¼šç”¨ generate_with_langchain_docs å¤„ç†å•ä¸ªæ–‡æ¡£ï¼ˆæ— åˆ†æ®µéœ€æ±‚ï¼‰
        print("ğŸ”„ å°è¯•æ–¹æ³•1ï¼šgenerate_with_langchain_docs")
        testset = generator.generate_with_langchain_docs(
            documents=documents,
            testset_size=TESTSET_SIZE
        )
        test_df = testset.to_pandas()
        test_df.rename(columns={
            'question': 'user_input',
            'ground_truth': 'reference'
        }, inplace=True)
        test_df['reference_contexts'] = [selected_texts] * len(test_df)

    except Exception as e:
        # ç¬¬äºŒæ¬¡å°è¯•ï¼šç›´æ¥ç”¨ LLM åŸºäºé•¿æ–‡æœ¬ç”Ÿæˆé—®é¢˜ï¼ˆç»•å¼€ Ragas å†…éƒ¨å¤„ç†ï¼‰
        print(f"\nâš ï¸  è‡ªåŠ¨é€‚é…æ—§ç‰ˆ Ragasï¼š{str(e)[:50]}")
        print("ğŸ”„ å°è¯•æ–¹æ³•2ï¼šæ‰‹åŠ¨é—®é¢˜ç”Ÿæˆå™¨")

        # æ‰‹åŠ¨åˆå§‹åŒ–é—®é¢˜ç”Ÿæˆå™¨ï¼Œç›´æ¥ç”Ÿæˆé—®é¢˜
        qg = QuestionGenerator(llm=generator_llm)
        questions = qg.generate(
            contexts=[combined_text],  # æ—§ç‰ˆæ”¯æŒçš„å‚æ•°å
            num_questions=TESTSET_SIZE
        )
        # æ„é€ æµ‹è¯•é›†æ ¼å¼ï¼ˆé€‚é…åç»­è¯„ä¼°ï¼‰
        test_data = {
            'user_input': questions,
            'reference': [combined_text[:500] + "..." for _ in questions],  # æˆªå–éƒ¨åˆ†ä½œä¸ºå‚è€ƒ
            'reference_contexts': [selected_texts for _ in questions]
        }
        test_df = pd.DataFrame(test_data)

    # 7. ä¿å­˜æµ‹è¯•é›†
    test_df.to_csv(TESTSET_FILE, index=False, encoding='utf-8-sig')

    print(f"\nğŸ‰ æµ‹è¯•é›†ç”Ÿæˆå®Œæ¯•ï¼å·²ä¿å­˜è‡³ {TESTSET_FILE}")
    print(f"ğŸ“Š å…±ç”Ÿæˆ {len(test_df)} æ¡æµ‹è¯•æ•°æ®")
    print("\né¢„è§ˆå‰2æ¡æ•°æ®ï¼š")
    print(test_df[['user_input', 'reference']].head(2))
    print("\nâœ… ä»»åŠ¡å®Œæˆï¼")


if __name__ == "__main__":
    main()

"""
æµ‹è¯•æ•°æ®ç”Ÿæˆæ¨¡å—
ä½œè€…ï¼šzjy
åˆ›å»ºæ—¶é—´ï¼š2024å¹´

è¯¥æ¨¡å—ç”¨äºä»å‘é‡æ•°æ®åº“ç”Ÿæˆæµ‹è¯•æ•°æ®é›†ï¼ŒåŒ…å«ä»¥ä¸‹åŠŸèƒ½ï¼š
1. ä»Chromaå‘é‡æ•°æ®åº“åŠ è½½æ–‡æ¡£æ•°æ®
2. ä½¿ç”¨K-Meansèšç±»ç­›é€‰ä»£è¡¨æ€§æ ·æœ¬
3. åŸºäºLLMç”Ÿæˆé—®ç­”å¯¹
4. è¾“å‡ºä¸ºCSVæ ¼å¼çš„æµ‹è¯•é›†

ä¸»è¦é…ç½®å‚æ•°ï¼š
- CHROMA_DB_DIR: å‘é‡æ•°æ®åº“è·¯å¾„
- TESTSET_FILE: è¾“å‡ºæµ‹è¯•é›†æ–‡ä»¶å
- N_CLUSTERS: K-Meansèšç±»æ•°é‡
- QUESTIONS_PER_DOC: æ¯ä¸ªæ–‡æ¡£ç”Ÿæˆçš„é—®é¢˜æ•°é‡
"""

import os
import warnings
from typing import List

import numpy as np
import pandas as pd
from langchain_chroma import Chroma
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import JsonOutputParser
from pydantic import BaseModel, Field
from sklearn.cluster import KMeans
from sklearn.metrics import pairwise_distances_argmin_min

from utils import get_llm_model, get_embeddings_model

# å¿½ç•¥è­¦å‘Šä¿¡æ¯
warnings.filterwarnings('ignore')


# ================= é…ç½®åŒºåŸŸ =================
CHROMA_DB_DIR = './data/db'
TESTSET_FILE = "auto_generated_testset.csv"
N_CLUSTERS = 20
QUESTIONS_PER_DOC = 1
# =========================================


class QAData(BaseModel):
    """
    é—®ç­”æ•°æ®ç»“æ„æ¨¡å‹

    ç”¨äºæ¥æ”¶å’ŒéªŒè¯LLMç”Ÿæˆçš„é—®ç­”å¯¹æ•°æ®ã€‚
    """
    question: str = Field(description="ç”Ÿæˆçš„æµ‹è¯•é—®é¢˜")
    answer: str = Field(description="é—®é¢˜çš„æ ‡å‡†ç­”æ¡ˆ")
    type: str = Field(description="é—®é¢˜ç±»å‹: simple æˆ– reasoning")


def main():
    """
    ä¸»å‡½æ•°ï¼šç”Ÿæˆæµ‹è¯•æ•°æ®é›†

    æµç¨‹ï¼š
    1. åˆå§‹åŒ–LLMå’ŒåµŒå…¥æ¨¡å‹
    2. ä»Chromaæ•°æ®åº“åŠ è½½æ–‡æ¡£
    3. ä½¿ç”¨K-Meansç­›é€‰ä»£è¡¨æ€§æ ·æœ¬
    4. ç”Ÿæˆé—®ç­”å¯¹
    5. ä¿å­˜ä¸ºCSVæ–‡ä»¶
    """
    print("ğŸš€ å¯åŠ¨å…œåº•æ–¹æ¡ˆï¼šç›´æ¥ä½¿ç”¨ LLM ç”Ÿæˆæµ‹è¯•é›† (å«æ ¼å¼è‡ªé€‚åº”ä¿®å¤)...")

    # åˆå§‹åŒ–æ¨¡å‹
    llm = get_llm_model()
    embedding_model = get_embeddings_model()

    print("ğŸ“– ä» Chroma åŠ è½½æ•°æ®...")
    vectorstore = Chroma(persist_directory=CHROMA_DB_DIR, embedding_function=embedding_model)
    db_data = vectorstore.get()
    texts = db_data['documents']

    if not texts:
        raise ValueError("âŒ æ•°æ®åº“ä¸ºç©ºï¼")

    # K-Means ç­›é€‰é€»è¾‘
    target_indices = []
    if len(texts) > N_CLUSTERS:
        print(f"âš¡ æ­£åœ¨ç­›é€‰ {N_CLUSTERS} ä¸ªä»£è¡¨æ€§ç‰‡æ®µ...")
        embeddings = vectorstore.get(include=['embeddings'])['embeddings']

        # åˆ¤ç©ºä¿®å¤
        if embeddings is None or len(embeddings) == 0:
            print("   è®¡ç®— Embeddings...")
            embeddings = embedding_model.embed_documents(texts)

        embeddings_np = np.array(embeddings)
        kmeans = KMeans(n_clusters=N_CLUSTERS, random_state=42, n_init=10)
        kmeans.fit(embeddings_np)
        closest, _ = pairwise_distances_argmin_min(kmeans.cluster_centers_, embeddings_np)
        target_indices = closest
    else:
        target_indices = range(len(texts))

    # åˆå§‹åŒ–è§£æå™¨
    parser = JsonOutputParser(pydantic_object=QAData)

    # å¼ºåŒ– Promptï¼Œæ˜ç¡®è¦æ±‚å•ä¸€å¯¹è±¡
    prompt = ChatPromptTemplate.from_messages([
        ("system", "ä½ æ˜¯ä¸€ä¸ª QA æ•°æ®é›†ç”Ÿæˆä¸“å®¶ã€‚è¯·æ ¹æ®ç”¨æˆ·æä¾›çš„æ–‡æœ¬ï¼Œç”Ÿæˆä¸€ä¸ª JSON å¯¹è±¡ã€‚\n"
                   "JSON å¿…é¡»åŒ…å« keys: 'question', 'answer', 'type'ã€‚\n"
                   "type åªèƒ½æ˜¯ 'simple' æˆ– 'reasoning'ã€‚\n"
                   "{format_instructions}"),
        ("human", "åŸæ–‡å†…å®¹ï¼š\n{context}\n\nè¯·ç”Ÿæˆ JSONï¼š")
    ])

    chain = prompt | llm | parser

    results = []
    print(f"ğŸ§  å¼€å§‹ç”Ÿæˆé—®é¢˜...")

    for i, idx in enumerate(target_indices):
        context_text = texts[idx]
        print(f"   Processing {i + 1}/{len(target_indices)}...", end="\r")

        try:
            response = chain.invoke({
                "context": context_text,
                "format_instructions": parser.get_format_instructions()
            })

            # å…¼å®¹åˆ—è¡¨å’Œå­—å…¸
            data_item = response

            # å¦‚æœè¿”å›çš„æ˜¯åˆ—è¡¨ [{}], å–ç¬¬ä¸€ä¸ªå…ƒç´ 
            if isinstance(response, list):
                if len(response) > 0:
                    data_item = response[0]
                else:
                    continue

            # å¦‚æœæ­¤æ—¶ data_item ä¸æ˜¯å­—å…¸ï¼Œè·³è¿‡
            if not isinstance(data_item, dict):
                print(f"\n   âš ï¸ ç¬¬ {i + 1} æ¡æ ¼å¼å¼‚å¸¸ (ç±»å‹: {type(data_item)}), è·³è¿‡...")
                continue

            # å®¹é”™å–å€¼ (é˜²æ­¢å¤§å°å†™å·®å¼‚)
            question = data_item.get('question') or data_item.get('Question')
            answer = data_item.get('answer') or data_item.get('Answer')
            q_type = data_item.get('type') or 'simple'

            if not question or not answer:
                print(f"\n   âš ï¸ ç¬¬ {i + 1} æ¡ç¼ºå°‘å¿…è¦å­—æ®µ, è·³è¿‡...")
                continue

            row = {
                'user_input': question,
                'reference': answer,
                'reference_contexts': [context_text],
                'type': q_type
            }
            results.append(row)

        except Exception as e:
            print(f"\n   âš ï¸ ç¬¬ {i + 1} æ¡ç”Ÿæˆå¤±è´¥: {str(e)}")
            continue

    if not results:
        print("\nâŒ ç”Ÿæˆå¤±è´¥ï¼Œæœªå¾—åˆ°ç»“æœã€‚")
        return

    df = pd.DataFrame(results)
    final_cols = ['user_input', 'reference', 'reference_contexts', 'type']
    save_cols = [c for c in final_cols if c in df.columns]

    df[save_cols].to_csv(TESTSET_FILE, index=False, encoding='utf-8-sig')

    print(f"\n\nğŸ‰ æˆåŠŸç”Ÿæˆ {len(df)} æ¡æ•°æ®ï¼")
    print(f"ğŸ“‚ å·²ä¿å­˜è‡³: {TESTSET_FILE}")


if __name__ == "__main__":
    main()
